{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tweets Samples\n",
    "\n",
    "The twitter_samples corpus contains 3 files.\n",
    "+ negative_tweets.json: contains 5k negative tweets\n",
    "+ positive_tweets.json: contains 5k positive tweets\n",
    "+ tweets.20150430-223406.json: contains 20k positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print(twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "\n",
    "print(len(pos_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print(len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "\n",
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "for tweet in pos_tweets[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Tweets\n",
    "\n",
    "Three different parameters can be passed while calling the TweetTokenizer class. They are:\n",
    "+ preserve_case: if False then it converts tweet to lowercase and vice-versa.\n",
    "+ strip_handles: if True then it removes twitter handles from the tweet and vice-versa.\n",
    "+ reduce_len: if True then it reduces the length of words in the tweet like hurrayyyy, yipppiieeee, etc. and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Tweets\n",
    "\n",
    "+ Remove stock market tickers like $GE\n",
    "+ Remove retweet text “RT”\n",
    "+ Remove hyperlinks\n",
    "+ Remove hashtags (only the hashtag # and not the word)\n",
    "+ Remove stop words like a, and, the, is, are, etc.\n",
    "+ Remove emoticons like :), :D, :(, :-), etc.\n",
    "+ Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "+ Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’, ‘works’, and ‘worked’ will be converted to their base/stem word “work”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    " \n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    " \n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []\n",
    "    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              word not in emoticons and # remove emoticons\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', 'good', 'morn']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n"
     ]
    }
   ],
   "source": [
    "print (pos_tweets[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'irresist', 'flipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "print (clean_tweets(pos_tweets[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We define a simple bag_of_words function that extracts unigram features from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': True, 'great': True, 'day': True, 'good': True, 'morn': True}\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "print (bag_of_words(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of positive:  5000\n",
      "Length of negative:  5000\n"
     ]
    }
   ],
   "source": [
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))\n",
    "    \n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "    \n",
    "print(\"Length of positive: \", len(pos_tweets_set)) \n",
    "print(\"Length of negative: \", len(neg_tweets_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Set\n",
    "\n",
    "There are 5000 positive tweets set and 5000 negative tweets set. We take 20% (i.e. 1000) of positive tweets and 20% (i.e. 1000) of negative tweets as the test set. The remaining negative and positive tweets will be taken as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "from random import shuffle \n",
    "\n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    " \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    " \n",
    "print(len(test_set),  len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7365\n",
      "Most Informative Features\n",
      "                     via = True              pos : neg    =     36.3 : 1.0\n",
      "                     bam = True              pos : neg    =     25.0 : 1.0\n",
      "                      aw = True              neg : pos    =     20.3 : 1.0\n",
      "                 appreci = True              pos : neg    =     18.3 : 1.0\n",
      "                     sad = True              neg : pos    =     18.1 : 1.0\n",
      "                     x15 = True              neg : pos    =     17.0 : 1.0\n",
      "                      ff = True              pos : neg    =     17.0 : 1.0\n",
      "                     ugh = True              neg : pos    =     15.7 : 1.0\n",
      "                    sick = True              neg : pos    =     13.8 : 1.0\n",
      "                  friday = True              pos : neg    =     13.5 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    "\n",
    "print (classifier.show_most_informative_features(10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Classifier with Custom Tweet\n",
    "\n",
    "We provide custom tweet and check the classification output of the trained classifier. The classifier correctly predicts both negative and positive tweets provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with custome review 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"Is that why you're so busy slashing Medicaid, giving tax cuts to the wealthy, put sexual predators on the bench, and allow kids to die?\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and classify custome review 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificate as:  neg\n",
      "Classified as:  <ProbDist with 2 samples>\n",
      "Classification category:  neg\n",
      "Negative probability :  0.9988870824411937\n",
      "Positive probability :  0.0011129175588098107\n"
     ]
    }
   ],
   "source": [
    "print(\"Classificate as: \", classifier.classify(custom_tweet_set))\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "\n",
    "print(\"Classified as: \", prob_result)\n",
    "print(\"Classification category: \", prob_result.max())\n",
    "print(\"Negative probability : \", prob_result.prob(\"neg\"))\n",
    "print(\"Positive probability : \", prob_result.prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with custome review 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and classify custome review 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificate as:  pos\n",
      "Classified as:  <ProbDist with 2 samples>\n",
      "Classification category:  pos\n",
      "Negative probability :  0.0004941345148866521\n",
      "Positive probability :  0.9995058654851143\n"
     ]
    }
   ],
   "source": [
    "print(\"Classificate as: \", classifier.classify(custom_tweet_set))\n",
    " \n",
    "# probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "\n",
    "print(\"Classified as: \", prob_result)\n",
    "print(\"Classification category: \", prob_result.max())\n",
    "print(\"Negative probability: \", prob_result.prob(\"neg\"))\n",
    "print(\"Positive probability: \", prob_result.prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos precision : 0.7345385347288297\n",
      "pos recall    : 0.772\n",
      "pos F-measure : 0.7528035104826915\n",
      "neg precision : 0.7597471022128557\n",
      "neg recall    : 0.721\n",
      "neg F-measure : 0.7398665982555156\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    " \n",
    "actual_set = defaultdict(set)\n",
    "predicted_set = defaultdict(set)\n",
    " \n",
    "actual_set_cm = []\n",
    "predicted_set_cm = []\n",
    " \n",
    "for index, (feature, actual_label) in enumerate(test_set):\n",
    "    actual_set[actual_label].add(index)\n",
    "    actual_set_cm.append(actual_label)\n",
    " \n",
    "    predicted_label = classifier.classify(feature)\n",
    " \n",
    "    predicted_set[predicted_label].add(index)\n",
    "    predicted_set_cm.append(predicted_label)\n",
    "    \n",
    "from nltk.metrics import precision, recall, f_measure, ConfusionMatrix\n",
    " \n",
    "print('pos precision :', precision(actual_set['pos'], predicted_set['pos'])) # Output: pos precision: 0.762896825397\n",
    "print('pos recall    :', recall(actual_set['pos'], predicted_set['pos'])) # Output: pos recall: 0.769\n",
    "print('pos F-measure :', f_measure(actual_set['pos'], predicted_set['pos'])) # Output: pos F-measure: 0.76593625498\n",
    " \n",
    "print('neg precision :', precision(actual_set['neg'], predicted_set['neg'])) # Output: neg precision: 0.767137096774\n",
    "print('neg recall    :', recall(actual_set['neg'], predicted_set['neg'])) # Output: neg recall: 0.761\n",
    "print('neg F-measure :', f_measure(actual_set['neg'], predicted_set['neg'])) # Output: neg F-measure: 0.7640562249"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "\n",
    "'''\n",
    "           |   Predicted NO      |   Predicted YES     |\n",
    "-----------+---------------------+---------------------+\n",
    "Actual NO  | True Negative (TN)  | False Positive (FP) |\n",
    "Actual YES | False Negative (FN) | True Positive (TP)  |\n",
    "-----------+---------------------+---------------------+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<721>279 |\n",
      "pos | 228<772>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(actual_set_cm, predicted_set_cm)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <36.0%> 13.9% |\n",
      "pos |  11.4% <38.6%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
