{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Step of NLP with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NLTK sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select gutenberg file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show gutenberg file content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:500]:\n",
    "    print(word, sep = ' ', end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "An process operating string into tokens which in turn are small structure or unit that can be used for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI=\"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\n",
    "\n",
    "Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.\n",
    "\n",
    "AI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, and then using the outcomes of this study as a basis of developing intelligent software and systems.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According',\n",
       " 'to',\n",
       " 'the',\n",
       " 'father',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'John',\n",
       " 'McCarthy',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " '“',\n",
       " 'The',\n",
       " 'science',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'of',\n",
       " 'making',\n",
       " 'intelligent',\n",
       " 'machines',\n",
       " ',',\n",
       " 'especially',\n",
       " 'intelligent',\n",
       " 'computer',\n",
       " 'programs',\n",
       " '”',\n",
       " '.',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'computer',\n",
       " ',',\n",
       " 'a',\n",
       " 'computer-controlled',\n",
       " 'robot',\n",
       " ',',\n",
       " 'or',\n",
       " 'a',\n",
       " 'software',\n",
       " 'think',\n",
       " 'intelligently',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'similar',\n",
       " 'manner',\n",
       " 'the',\n",
       " 'intelligent',\n",
       " 'humans',\n",
       " 'think',\n",
       " '.',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'accomplished',\n",
       " 'by',\n",
       " 'studying',\n",
       " 'how',\n",
       " 'human',\n",
       " 'brain',\n",
       " 'thinks',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'humans',\n",
       " 'learn',\n",
       " ',',\n",
       " 'decide',\n",
       " ',',\n",
       " 'and',\n",
       " 'work',\n",
       " 'while',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'a',\n",
       " 'problem',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'using',\n",
       " 'the',\n",
       " 'outcomes',\n",
       " 'of',\n",
       " 'this',\n",
       " 'study',\n",
       " 'as',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'of',\n",
       " 'developing',\n",
       " 'intelligent',\n",
       " 'software',\n",
       " 'and',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "\n",
    "AI_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count all the words in paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 10, 'a': 6, 'the': 5, 'of': 5, 'and': 5, 'intelligent': 4, 'is': 3, '.': 3, 'to': 2, 'artificial': 2, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 10),\n",
       " ('a', 6),\n",
       " ('the', 5),\n",
       " ('of', 5),\n",
       " ('and', 5),\n",
       " ('intelligent', 4),\n",
       " ('is', 3),\n",
       " ('.', 3),\n",
       " ('to', 2),\n",
       " ('artificial', 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top10 = fdist.most_common(10)\n",
    "\n",
    "fdist_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count blank paragraph (Black tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize\n",
    "\n",
    "AI_blank=blankline_tokenize(AI)\n",
    "\n",
    "len(AI_blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important parts of tokenization:\n",
    "\n",
    "+ Bigram: Token of two consecutive written words\n",
    "+ Trigram: Token of three consecutive written words\n",
    "+ Ngram: Tokens of any number of consecutive written words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'best',\n",
       " 'and',\n",
       " 'most',\n",
       " 'beautiful',\n",
       " 'things',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'seen',\n",
       " 'or',\n",
       " 'even',\n",
       " 'touched',\n",
       " ',',\n",
       " 'they',\n",
       " 'must',\n",
       " 'be',\n",
       " 'felt',\n",
       " 'with',\n",
       " 'heart']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"The best and most beautiful things in the world cannot be seen or even touched, they must be felt with heart\"\n",
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "\n",
    "quotes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best'),\n",
       " ('best', 'and'),\n",
       " ('and', 'most'),\n",
       " ('most', 'beautiful'),\n",
       " ('beautiful', 'things'),\n",
       " ('things', 'in'),\n",
       " ('in', 'the'),\n",
       " ('the', 'world'),\n",
       " ('world', 'can'),\n",
       " ('can', 'not'),\n",
       " ('not', 'be'),\n",
       " ('be', 'seen'),\n",
       " ('seen', 'or'),\n",
       " ('or', 'even'),\n",
       " ('even', 'touched'),\n",
       " ('touched', ','),\n",
       " (',', 'they'),\n",
       " ('they', 'must'),\n",
       " ('must', 'be'),\n",
       " ('be', 'felt'),\n",
       " ('felt', 'with'),\n",
       " ('with', 'heart')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "\n",
    "quotes_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and'),\n",
       " ('best', 'and', 'most'),\n",
       " ('and', 'most', 'beautiful'),\n",
       " ('most', 'beautiful', 'things'),\n",
       " ('beautiful', 'things', 'in'),\n",
       " ('things', 'in', 'the'),\n",
       " ('in', 'the', 'world'),\n",
       " ('the', 'world', 'can'),\n",
       " ('world', 'can', 'not'),\n",
       " ('can', 'not', 'be'),\n",
       " ('not', 'be', 'seen'),\n",
       " ('be', 'seen', 'or'),\n",
       " ('seen', 'or', 'even'),\n",
       " ('or', 'even', 'touched'),\n",
       " ('even', 'touched', ','),\n",
       " ('touched', ',', 'they'),\n",
       " (',', 'they', 'must'),\n",
       " ('they', 'must', 'be'),\n",
       " ('must', 'be', 'felt'),\n",
       " ('be', 'felt', 'with'),\n",
       " ('felt', 'with', 'heart')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "\n",
    "quotes_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'best', 'and', 'most', 'beautiful'),\n",
       " ('best', 'and', 'most', 'beautiful', 'things'),\n",
       " ('and', 'most', 'beautiful', 'things', 'in'),\n",
       " ('most', 'beautiful', 'things', 'in', 'the'),\n",
       " ('beautiful', 'things', 'in', 'the', 'world'),\n",
       " ('things', 'in', 'the', 'world', 'can'),\n",
       " ('in', 'the', 'world', 'can', 'not'),\n",
       " ('the', 'world', 'can', 'not', 'be'),\n",
       " ('world', 'can', 'not', 'be', 'seen'),\n",
       " ('can', 'not', 'be', 'seen', 'or'),\n",
       " ('not', 'be', 'seen', 'or', 'even'),\n",
       " ('be', 'seen', 'or', 'even', 'touched'),\n",
       " ('seen', 'or', 'even', 'touched', ','),\n",
       " ('or', 'even', 'touched', ',', 'they'),\n",
       " ('even', 'touched', ',', 'they', 'must'),\n",
       " ('touched', ',', 'they', 'must', 'be'),\n",
       " (',', 'they', 'must', 'be', 'felt'),\n",
       " ('they', 'must', 'be', 'felt', 'with'),\n",
       " ('must', 'be', 'felt', 'with', 'heart')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\n",
    "\n",
    "quotes_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "Normalize words into its bas form or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: given\n",
      "gave: gave\n"
     ]
    }
   ],
   "source": [
    "words_to_stem1=[\"give\",\"giving\",\"given\",\"gave\"]\n",
    "words_to_stem2=[\"go\",\"going\",\"went\",\"gone\"]\n",
    "\n",
    "for words in words_to_stem1:\n",
    "    print(words+ \": \" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "going: go\n",
      "went: went\n",
      "gone: gone\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem2:\n",
    "    print(words+ \": \" +pst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: giv\n",
      "giving: giv\n",
      "given: giv\n",
      "gave: gav\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "lst=LancasterStemmer()\n",
    "\n",
    "for words in words_to_stem1:\n",
    "    print(words+ \": \" +lst.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "going: going\n",
      "went: went\n",
      "gone: gon\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem2:\n",
    "    print(words+ \": \" +lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LancasterStemmer** is more agresive than **Porterstemmer**. It usually for counting the used words. There're other stemmer are like **SnowBostonStemmer** which are using to specify the language are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "Are use to considerate the morphological analysis of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giving: give\n",
      "given: give\n",
      "gave: give\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_lem=WordNetLemmatizer()\n",
    "\n",
    "words_to_stem3=[\"do\",\"did\",\"done\"]\n",
    "\n",
    "for words in words_to_stem1:\n",
    "    print(words+ \": \" +word_lem.lemmatize(words, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go: go\n",
      "going: go\n",
      "went: go\n",
      "gone: go\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem2:\n",
    "    print(words+ \": \" +word_lem.lemmatize(words, 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do: do\n",
      "did: do\n",
      "done: do\n"
     ]
    }
   ],
   "source": [
    "for words in words_to_stem3:\n",
    "    print(words+ \": \" +word_lem.lemmatize(words, 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tag\n",
    "\n",
    "Word or sentences component, such as **noun**, **verb** or **object**.\n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Very useful english language, such as I, your, were, bellow, above and etc. Which are very usefull in sentences, but this word do not provide any help in NLP (Processing Languages). NLTK has the own stop word, and we just neet to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ada',\n",
       " 'adalah',\n",
       " 'adanya',\n",
       " 'adapun',\n",
       " 'agak',\n",
       " 'agaknya',\n",
       " 'agar',\n",
       " 'akan',\n",
       " 'akankah',\n",
       " 'akhir',\n",
       " 'akhiri',\n",
       " 'akhirnya',\n",
       " 'aku',\n",
       " 'akulah',\n",
       " 'amat',\n",
       " 'amatlah',\n",
       " 'anda',\n",
       " 'andalah',\n",
       " 'antar',\n",
       " 'antara',\n",
       " 'antaranya',\n",
       " 'apa',\n",
       " 'apaan',\n",
       " 'apabila',\n",
       " 'apakah',\n",
       " 'apalagi',\n",
       " 'apatah',\n",
       " 'artinya',\n",
       " 'asal',\n",
       " 'asalkan',\n",
       " 'atas',\n",
       " 'atau',\n",
       " 'ataukah',\n",
       " 'ataupun',\n",
       " 'awal',\n",
       " 'awalnya',\n",
       " 'bagai',\n",
       " 'bagaikan',\n",
       " 'bagaimana',\n",
       " 'bagaimanakah',\n",
       " 'bagaimanapun',\n",
       " 'bagi',\n",
       " 'bagian',\n",
       " 'bahkan',\n",
       " 'bahwa',\n",
       " 'bahwasanya',\n",
       " 'baik',\n",
       " 'bakal',\n",
       " 'bakalan',\n",
       " 'balik',\n",
       " 'banyak',\n",
       " 'bapak',\n",
       " 'baru',\n",
       " 'bawah',\n",
       " 'beberapa',\n",
       " 'begini',\n",
       " 'beginian',\n",
       " 'beginikah',\n",
       " 'beginilah',\n",
       " 'begitu',\n",
       " 'begitukah',\n",
       " 'begitulah',\n",
       " 'begitupun',\n",
       " 'bekerja',\n",
       " 'belakang',\n",
       " 'belakangan',\n",
       " 'belum',\n",
       " 'belumlah',\n",
       " 'benar',\n",
       " 'benarkah',\n",
       " 'benarlah',\n",
       " 'berada',\n",
       " 'berakhir',\n",
       " 'berakhirlah',\n",
       " 'berakhirnya',\n",
       " 'berapa',\n",
       " 'berapakah',\n",
       " 'berapalah',\n",
       " 'berapapun',\n",
       " 'berarti',\n",
       " 'berawal',\n",
       " 'berbagai',\n",
       " 'berdatangan',\n",
       " 'beri',\n",
       " 'berikan',\n",
       " 'berikut',\n",
       " 'berikutnya',\n",
       " 'berjumlah',\n",
       " 'berkali-kali',\n",
       " 'berkata',\n",
       " 'berkehendak',\n",
       " 'berkeinginan',\n",
       " 'berkenaan',\n",
       " 'berlainan',\n",
       " 'berlalu',\n",
       " 'berlangsung',\n",
       " 'berlebihan',\n",
       " 'bermacam',\n",
       " 'bermacam-macam',\n",
       " 'bermaksud',\n",
       " 'bermula',\n",
       " 'bersama',\n",
       " 'bersama-sama',\n",
       " 'bersiap',\n",
       " 'bersiap-siap',\n",
       " 'bertanya',\n",
       " 'bertanya-tanya',\n",
       " 'berturut',\n",
       " 'berturut-turut',\n",
       " 'bertutur',\n",
       " 'berujar',\n",
       " 'berupa',\n",
       " 'besar',\n",
       " 'betul',\n",
       " 'betulkah',\n",
       " 'biasa',\n",
       " 'biasanya',\n",
       " 'bila',\n",
       " 'bilakah',\n",
       " 'bisa',\n",
       " 'bisakah',\n",
       " 'boleh',\n",
       " 'bolehkah',\n",
       " 'bolehlah',\n",
       " 'buat',\n",
       " 'bukan',\n",
       " 'bukankah',\n",
       " 'bukanlah',\n",
       " 'bukannya',\n",
       " 'bulan',\n",
       " 'bung',\n",
       " 'cara',\n",
       " 'caranya',\n",
       " 'cukup',\n",
       " 'cukupkah',\n",
       " 'cukuplah',\n",
       " 'cuma',\n",
       " 'dahulu',\n",
       " 'dalam',\n",
       " 'dan',\n",
       " 'dapat',\n",
       " 'dari',\n",
       " 'daripada',\n",
       " 'datang',\n",
       " 'dekat',\n",
       " 'demi',\n",
       " 'demikian',\n",
       " 'demikianlah',\n",
       " 'dengan',\n",
       " 'depan',\n",
       " 'di',\n",
       " 'dia',\n",
       " 'diakhiri',\n",
       " 'diakhirinya',\n",
       " 'dialah',\n",
       " 'diantara',\n",
       " 'diantaranya',\n",
       " 'diberi',\n",
       " 'diberikan',\n",
       " 'diberikannya',\n",
       " 'dibuat',\n",
       " 'dibuatnya',\n",
       " 'didapat',\n",
       " 'didatangkan',\n",
       " 'digunakan',\n",
       " 'diibaratkan',\n",
       " 'diibaratkannya',\n",
       " 'diingat',\n",
       " 'diingatkan',\n",
       " 'diinginkan',\n",
       " 'dijawab',\n",
       " 'dijelaskan',\n",
       " 'dijelaskannya',\n",
       " 'dikarenakan',\n",
       " 'dikatakan',\n",
       " 'dikatakannya',\n",
       " 'dikerjakan',\n",
       " 'diketahui',\n",
       " 'diketahuinya',\n",
       " 'dikira',\n",
       " 'dilakukan',\n",
       " 'dilalui',\n",
       " 'dilihat',\n",
       " 'dimaksud',\n",
       " 'dimaksudkan',\n",
       " 'dimaksudkannya',\n",
       " 'dimaksudnya',\n",
       " 'diminta',\n",
       " 'dimintai',\n",
       " 'dimisalkan',\n",
       " 'dimulai',\n",
       " 'dimulailah',\n",
       " 'dimulainya',\n",
       " 'dimungkinkan',\n",
       " 'dini',\n",
       " 'dipastikan',\n",
       " 'diperbuat',\n",
       " 'diperbuatnya',\n",
       " 'dipergunakan',\n",
       " 'diperkirakan',\n",
       " 'diperlihatkan',\n",
       " 'diperlukan',\n",
       " 'diperlukannya',\n",
       " 'dipersoalkan',\n",
       " 'dipertanyakan',\n",
       " 'dipunyai',\n",
       " 'diri',\n",
       " 'dirinya',\n",
       " 'disampaikan',\n",
       " 'disebut',\n",
       " 'disebutkan',\n",
       " 'disebutkannya',\n",
       " 'disini',\n",
       " 'disinilah',\n",
       " 'ditambahkan',\n",
       " 'ditandaskan',\n",
       " 'ditanya',\n",
       " 'ditanyai',\n",
       " 'ditanyakan',\n",
       " 'ditegaskan',\n",
       " 'ditujukan',\n",
       " 'ditunjuk',\n",
       " 'ditunjuki',\n",
       " 'ditunjukkan',\n",
       " 'ditunjukkannya',\n",
       " 'ditunjuknya',\n",
       " 'dituturkan',\n",
       " 'dituturkannya',\n",
       " 'diucapkan',\n",
       " 'diucapkannya',\n",
       " 'diungkapkan',\n",
       " 'dong',\n",
       " 'dua',\n",
       " 'dulu',\n",
       " 'empat',\n",
       " 'enggak',\n",
       " 'enggaknya',\n",
       " 'entah',\n",
       " 'entahlah',\n",
       " 'guna',\n",
       " 'gunakan',\n",
       " 'hal',\n",
       " 'hampir',\n",
       " 'hanya',\n",
       " 'hanyalah',\n",
       " 'hari',\n",
       " 'harus',\n",
       " 'haruslah',\n",
       " 'harusnya',\n",
       " 'hendak',\n",
       " 'hendaklah',\n",
       " 'hendaknya',\n",
       " 'hingga',\n",
       " 'ia',\n",
       " 'ialah',\n",
       " 'ibarat',\n",
       " 'ibaratkan',\n",
       " 'ibaratnya',\n",
       " 'ibu',\n",
       " 'ikut',\n",
       " 'ingat',\n",
       " 'ingat-ingat',\n",
       " 'ingin',\n",
       " 'inginkah',\n",
       " 'inginkan',\n",
       " 'ini',\n",
       " 'inikah',\n",
       " 'inilah',\n",
       " 'itu',\n",
       " 'itukah',\n",
       " 'itulah',\n",
       " 'jadi',\n",
       " 'jadilah',\n",
       " 'jadinya',\n",
       " 'jangan',\n",
       " 'jangankan',\n",
       " 'janganlah',\n",
       " 'jauh',\n",
       " 'jawab',\n",
       " 'jawaban',\n",
       " 'jawabnya',\n",
       " 'jelas',\n",
       " 'jelaskan',\n",
       " 'jelaslah',\n",
       " 'jelasnya',\n",
       " 'jika',\n",
       " 'jikalau',\n",
       " 'juga',\n",
       " 'jumlah',\n",
       " 'jumlahnya',\n",
       " 'justru',\n",
       " 'kala',\n",
       " 'kalau',\n",
       " 'kalaulah',\n",
       " 'kalaupun',\n",
       " 'kalian',\n",
       " 'kami',\n",
       " 'kamilah',\n",
       " 'kamu',\n",
       " 'kamulah',\n",
       " 'kan',\n",
       " 'kapan',\n",
       " 'kapankah',\n",
       " 'kapanpun',\n",
       " 'karena',\n",
       " 'karenanya',\n",
       " 'kasus',\n",
       " 'kata',\n",
       " 'katakan',\n",
       " 'katakanlah',\n",
       " 'katanya',\n",
       " 'ke',\n",
       " 'keadaan',\n",
       " 'kebetulan',\n",
       " 'kecil',\n",
       " 'kedua',\n",
       " 'keduanya',\n",
       " 'keinginan',\n",
       " 'kelamaan',\n",
       " 'kelihatan',\n",
       " 'kelihatannya',\n",
       " 'kelima',\n",
       " 'keluar',\n",
       " 'kembali',\n",
       " 'kemudian',\n",
       " 'kemungkinan',\n",
       " 'kemungkinannya',\n",
       " 'kenapa',\n",
       " 'kepada',\n",
       " 'kepadanya',\n",
       " 'kesampaian',\n",
       " 'keseluruhan',\n",
       " 'keseluruhannya',\n",
       " 'keterlaluan',\n",
       " 'ketika',\n",
       " 'khususnya',\n",
       " 'kini',\n",
       " 'kinilah',\n",
       " 'kira',\n",
       " 'kira-kira',\n",
       " 'kiranya',\n",
       " 'kita',\n",
       " 'kitalah',\n",
       " 'kok',\n",
       " 'kurang',\n",
       " 'lagi',\n",
       " 'lagian',\n",
       " 'lah',\n",
       " 'lain',\n",
       " 'lainnya',\n",
       " 'lalu',\n",
       " 'lama',\n",
       " 'lamanya',\n",
       " 'lanjut',\n",
       " 'lanjutnya',\n",
       " 'lebih',\n",
       " 'lewat',\n",
       " 'lima',\n",
       " 'luar',\n",
       " 'macam',\n",
       " 'maka',\n",
       " 'makanya',\n",
       " 'makin',\n",
       " 'malah',\n",
       " 'malahan',\n",
       " 'mampu',\n",
       " 'mampukah',\n",
       " 'mana',\n",
       " 'manakala',\n",
       " 'manalagi',\n",
       " 'masa',\n",
       " 'masalah',\n",
       " 'masalahnya',\n",
       " 'masih',\n",
       " 'masihkah',\n",
       " 'masing',\n",
       " 'masing-masing',\n",
       " 'mau',\n",
       " 'maupun',\n",
       " 'melainkan',\n",
       " 'melakukan',\n",
       " 'melalui',\n",
       " 'melihat',\n",
       " 'melihatnya',\n",
       " 'memang',\n",
       " 'memastikan',\n",
       " 'memberi',\n",
       " 'memberikan',\n",
       " 'membuat',\n",
       " 'memerlukan',\n",
       " 'memihak',\n",
       " 'meminta',\n",
       " 'memintakan',\n",
       " 'memisalkan',\n",
       " 'memperbuat',\n",
       " 'mempergunakan',\n",
       " 'memperkirakan',\n",
       " 'memperlihatkan',\n",
       " 'mempersiapkan',\n",
       " 'mempersoalkan',\n",
       " 'mempertanyakan',\n",
       " 'mempunyai',\n",
       " 'memulai',\n",
       " 'memungkinkan',\n",
       " 'menaiki',\n",
       " 'menambahkan',\n",
       " 'menandaskan',\n",
       " 'menanti',\n",
       " 'menanti-nanti',\n",
       " 'menantikan',\n",
       " 'menanya',\n",
       " 'menanyai',\n",
       " 'menanyakan',\n",
       " 'mendapat',\n",
       " 'mendapatkan',\n",
       " 'mendatang',\n",
       " 'mendatangi',\n",
       " 'mendatangkan',\n",
       " 'menegaskan',\n",
       " 'mengakhiri',\n",
       " 'mengapa',\n",
       " 'mengatakan',\n",
       " 'mengatakannya',\n",
       " 'mengenai',\n",
       " 'mengerjakan',\n",
       " 'mengetahui',\n",
       " 'menggunakan',\n",
       " 'menghendaki',\n",
       " 'mengibaratkan',\n",
       " 'mengibaratkannya',\n",
       " 'mengingat',\n",
       " 'mengingatkan',\n",
       " 'menginginkan',\n",
       " 'mengira',\n",
       " 'mengucapkan',\n",
       " 'mengucapkannya',\n",
       " 'mengungkapkan',\n",
       " 'menjadi',\n",
       " 'menjawab',\n",
       " 'menjelaskan',\n",
       " 'menuju',\n",
       " 'menunjuk',\n",
       " 'menunjuki',\n",
       " 'menunjukkan',\n",
       " 'menunjuknya',\n",
       " 'menurut',\n",
       " 'menuturkan',\n",
       " 'menyampaikan',\n",
       " 'menyangkut',\n",
       " 'menyatakan',\n",
       " 'menyebutkan',\n",
       " 'menyeluruh',\n",
       " 'menyiapkan',\n",
       " 'merasa',\n",
       " 'mereka',\n",
       " 'merekalah',\n",
       " 'merupakan',\n",
       " 'meski',\n",
       " 'meskipun',\n",
       " 'meyakini',\n",
       " 'meyakinkan',\n",
       " 'minta',\n",
       " 'mirip',\n",
       " 'misal',\n",
       " 'misalkan',\n",
       " 'misalnya',\n",
       " 'mula',\n",
       " 'mulai',\n",
       " 'mulailah',\n",
       " 'mulanya',\n",
       " 'mungkin',\n",
       " 'mungkinkah',\n",
       " 'nah',\n",
       " 'naik',\n",
       " 'namun',\n",
       " 'nanti',\n",
       " 'nantinya',\n",
       " 'nyaris',\n",
       " 'nyatanya',\n",
       " 'oleh',\n",
       " 'olehnya',\n",
       " 'pada',\n",
       " 'padahal',\n",
       " 'padanya',\n",
       " 'pak',\n",
       " 'paling',\n",
       " 'panjang',\n",
       " 'pantas',\n",
       " 'para',\n",
       " 'pasti',\n",
       " 'pastilah',\n",
       " 'penting',\n",
       " 'pentingnya',\n",
       " 'per',\n",
       " 'percuma',\n",
       " 'perlu',\n",
       " 'perlukah',\n",
       " 'perlunya',\n",
       " 'pernah',\n",
       " 'persoalan',\n",
       " 'pertama',\n",
       " 'pertama-tama',\n",
       " 'pertanyaan',\n",
       " 'pertanyakan',\n",
       " 'pihak',\n",
       " 'pihaknya',\n",
       " 'pukul',\n",
       " 'pula',\n",
       " 'pun',\n",
       " 'punya',\n",
       " 'rasa',\n",
       " 'rasanya',\n",
       " 'rata',\n",
       " 'rupanya',\n",
       " 'saat',\n",
       " 'saatnya',\n",
       " 'saja',\n",
       " 'sajalah',\n",
       " 'saling',\n",
       " 'sama',\n",
       " 'sama-sama',\n",
       " 'sambil',\n",
       " 'sampai',\n",
       " 'sampai-sampai',\n",
       " 'sampaikan',\n",
       " 'sana',\n",
       " 'sangat',\n",
       " 'sangatlah',\n",
       " 'satu',\n",
       " 'saya',\n",
       " 'sayalah',\n",
       " 'se',\n",
       " 'sebab',\n",
       " 'sebabnya',\n",
       " 'sebagai',\n",
       " 'sebagaimana',\n",
       " 'sebagainya',\n",
       " 'sebagian',\n",
       " 'sebaik',\n",
       " 'sebaik-baiknya',\n",
       " 'sebaiknya',\n",
       " 'sebaliknya',\n",
       " 'sebanyak',\n",
       " 'sebegini',\n",
       " 'sebegitu',\n",
       " 'sebelum',\n",
       " 'sebelumnya',\n",
       " 'sebenarnya',\n",
       " 'seberapa',\n",
       " 'sebesar',\n",
       " 'sebetulnya',\n",
       " 'sebisanya',\n",
       " 'sebuah',\n",
       " 'sebut',\n",
       " 'sebutlah',\n",
       " 'sebutnya',\n",
       " 'secara',\n",
       " 'secukupnya',\n",
       " 'sedang',\n",
       " 'sedangkan',\n",
       " 'sedemikian',\n",
       " 'sedikit',\n",
       " 'sedikitnya',\n",
       " 'seenaknya',\n",
       " 'segala',\n",
       " 'segalanya',\n",
       " 'segera',\n",
       " 'seharusnya',\n",
       " 'sehingga',\n",
       " 'seingat',\n",
       " 'sejak',\n",
       " 'sejauh',\n",
       " 'sejenak',\n",
       " 'sejumlah',\n",
       " 'sekadar',\n",
       " 'sekadarnya',\n",
       " 'sekali',\n",
       " 'sekali-kali',\n",
       " 'sekalian',\n",
       " 'sekaligus',\n",
       " 'sekalipun',\n",
       " 'sekarang',\n",
       " 'sekarang',\n",
       " 'sekecil',\n",
       " 'seketika',\n",
       " 'sekiranya',\n",
       " 'sekitar',\n",
       " 'sekitarnya',\n",
       " 'sekurang-kurangnya',\n",
       " 'sekurangnya',\n",
       " 'sela',\n",
       " 'selain',\n",
       " 'selaku',\n",
       " 'selalu',\n",
       " 'selama',\n",
       " 'selama-lamanya',\n",
       " 'selamanya',\n",
       " 'selanjutnya',\n",
       " 'seluruh',\n",
       " 'seluruhnya',\n",
       " 'semacam',\n",
       " 'semakin',\n",
       " 'semampu',\n",
       " 'semampunya',\n",
       " 'semasa',\n",
       " 'semasih',\n",
       " 'semata',\n",
       " 'semata-mata',\n",
       " 'semaunya',\n",
       " 'sementara',\n",
       " 'semisal',\n",
       " 'semisalnya',\n",
       " 'sempat',\n",
       " 'semua',\n",
       " 'semuanya',\n",
       " 'semula',\n",
       " 'sendiri',\n",
       " 'sendirian',\n",
       " 'sendirinya',\n",
       " 'seolah',\n",
       " 'seolah-olah',\n",
       " 'seorang',\n",
       " 'sepanjang',\n",
       " 'sepantasnya',\n",
       " 'sepantasnyalah',\n",
       " 'seperlunya',\n",
       " 'seperti',\n",
       " 'sepertinya',\n",
       " 'sepihak',\n",
       " 'sering',\n",
       " 'seringnya',\n",
       " 'serta',\n",
       " 'serupa',\n",
       " 'sesaat',\n",
       " 'sesama',\n",
       " 'sesampai',\n",
       " 'sesegera',\n",
       " 'sesekali',\n",
       " 'seseorang',\n",
       " 'sesuatu',\n",
       " 'sesuatunya',\n",
       " 'sesudah',\n",
       " 'sesudahnya',\n",
       " 'setelah',\n",
       " 'setempat',\n",
       " 'setengah',\n",
       " 'seterusnya',\n",
       " 'setiap',\n",
       " 'setiba',\n",
       " 'setibanya',\n",
       " 'setidak-tidaknya',\n",
       " 'setidaknya',\n",
       " 'setinggi',\n",
       " 'seusai',\n",
       " 'sewaktu',\n",
       " 'siap',\n",
       " 'siapa',\n",
       " 'siapakah',\n",
       " 'siapapun',\n",
       " 'sini',\n",
       " 'sinilah',\n",
       " 'soal',\n",
       " 'soalnya',\n",
       " 'suatu',\n",
       " 'sudah',\n",
       " 'sudahkah',\n",
       " 'sudahlah',\n",
       " 'supaya',\n",
       " 'tadi',\n",
       " 'tadinya',\n",
       " 'tahu',\n",
       " 'tahun',\n",
       " 'tak',\n",
       " 'tambah',\n",
       " 'tambahnya',\n",
       " 'tampak',\n",
       " 'tampaknya',\n",
       " 'tandas',\n",
       " 'tandasnya',\n",
       " 'tanpa',\n",
       " 'tanya',\n",
       " 'tanyakan',\n",
       " 'tanyanya',\n",
       " 'tapi',\n",
       " 'tegas',\n",
       " 'tegasnya',\n",
       " 'telah',\n",
       " 'tempat',\n",
       " 'tengah',\n",
       " 'tentang',\n",
       " 'tentu',\n",
       " 'tentulah',\n",
       " 'tentunya',\n",
       " 'tepat',\n",
       " 'terakhir',\n",
       " 'terasa',\n",
       " 'terbanyak',\n",
       " 'terdahulu',\n",
       " 'terdapat',\n",
       " 'terdiri',\n",
       " 'terhadap',\n",
       " 'terhadapnya',\n",
       " 'teringat',\n",
       " 'teringat-ingat',\n",
       " 'terjadi',\n",
       " 'terjadilah',\n",
       " 'terjadinya',\n",
       " 'terkira',\n",
       " 'terlalu',\n",
       " 'terlebih',\n",
       " 'terlihat',\n",
       " 'termasuk',\n",
       " 'ternyata',\n",
       " 'tersampaikan',\n",
       " 'tersebut',\n",
       " 'tersebutlah',\n",
       " 'tertentu',\n",
       " 'tertuju',\n",
       " 'terus',\n",
       " 'terutama',\n",
       " 'tetap',\n",
       " 'tetapi',\n",
       " 'tiap',\n",
       " 'tiba',\n",
       " 'tiba-tiba',\n",
       " 'tidak',\n",
       " 'tidakkah',\n",
       " 'tidaklah',\n",
       " 'tiga',\n",
       " 'tinggi',\n",
       " 'toh',\n",
       " 'tunjuk',\n",
       " 'turut',\n",
       " 'tutur',\n",
       " 'tuturnya',\n",
       " 'ucap',\n",
       " 'ucapnya',\n",
       " 'ujar',\n",
       " 'ujarnya',\n",
       " 'umum',\n",
       " 'umumnya',\n",
       " 'ungkap',\n",
       " 'ungkapnya',\n",
       " 'untuk',\n",
       " 'usah',\n",
       " 'usai',\n",
       " 'waduh',\n",
       " 'wah',\n",
       " 'wahai',\n",
       " 'waktu',\n",
       " 'waktunya',\n",
       " 'walau',\n",
       " 'walaupun',\n",
       " 'wong',\n",
       " 'yaitu',\n",
       " 'yakin',\n",
       " 'yakni',\n",
       " 'yang']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to remove the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "punctuation=re.compile(r'[-.?!.:;()|0-9]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According',\n",
       " 'to',\n",
       " 'the',\n",
       " 'father',\n",
       " 'of',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " ',',\n",
       " 'John',\n",
       " 'McCarthy',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " '“',\n",
       " 'The',\n",
       " 'science',\n",
       " 'and',\n",
       " 'engineering',\n",
       " 'of',\n",
       " 'making',\n",
       " 'intelligent',\n",
       " 'machines',\n",
       " ',',\n",
       " 'especially',\n",
       " 'intelligent',\n",
       " 'computer',\n",
       " 'programs',\n",
       " '”',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'way',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'computer',\n",
       " ',',\n",
       " 'a',\n",
       " 'computercontrolled',\n",
       " 'robot',\n",
       " ',',\n",
       " 'or',\n",
       " 'a',\n",
       " 'software',\n",
       " 'think',\n",
       " 'intelligently',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'similar',\n",
       " 'manner',\n",
       " 'the',\n",
       " 'intelligent',\n",
       " 'humans',\n",
       " 'think',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'accomplished',\n",
       " 'by',\n",
       " 'studying',\n",
       " 'how',\n",
       " 'human',\n",
       " 'brain',\n",
       " 'thinks',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'humans',\n",
       " 'learn',\n",
       " ',',\n",
       " 'decide',\n",
       " ',',\n",
       " 'and',\n",
       " 'work',\n",
       " 'while',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'a',\n",
       " 'problem',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'using',\n",
       " 'the',\n",
       " 'outcomes',\n",
       " 'of',\n",
       " 'this',\n",
       " 'study',\n",
       " 'as',\n",
       " 'a',\n",
       " 'basis',\n",
       " 'of',\n",
       " 'developing',\n",
       " 'intelligent',\n",
       " 'software',\n",
       " 'and',\n",
       " 'systems']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_punctuation=[]\n",
    "\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\", words)\n",
    "    \n",
    "    if len(word) > 0:\n",
    "        post_punctuation.append(word)\n",
    "        \n",
    "post_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(post_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Remove unecessary word which don't have much meaning is very inpoortant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech\n",
    "\n",
    "Most important component of the sentence are s, subect, verb, object, noun, adjective, adverb or etc. There're so many important component, so we must setup tags for POS (POS TAGS). Here's the example of POS TAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Timothy is a natural when it comes to drawing\"\n",
    "sent_tokens = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"John is eating a delicious cake\"\n",
    "sent_tokens2 = word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens2:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Agus', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('eating', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('delicious', 'JJ')]\n",
      "[('cake', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent3 = \"Agus is eating a delicious cake\"\n",
    "sent_tokens3 = word_tokenize(sent3)\n",
    "\n",
    "for token in sent_tokens3:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "The process detecting the named entities such as the person names, location names, company names, organization names, quantities and the monetary values was called entity recognition.\n",
    "\n",
    "There are three types of a indentification:\n",
    "+ **Noun pharase identification**: work to extracting all noun phrases from a text using dependency passing and part of speech tagging.\n",
    "+ **Phrase classification**: this is the classification step in which all the extracted noun phrases are classified into respective categories which are the location, organization and much more. This classification can create the look-up tables and dictionaries system by combining information from different sources.\n",
    "+ **Entity Ambiguation**: sometimes is possible the entities are misclassified when creating a validation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_sent1=\"The US President stays in the WHITE HOUSE\"\n",
    "NE_sent2=\"Google's CEO Sundar Pichai introduce the new Pixel 3 at New York Central Mall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_tokens1=word_tokenize(NE_sent1)\n",
    "NE_tags1=nltk.pos_tag(NE_tokens1)\n",
    "\n",
    "NE_tokens2=word_tokenize(NE_sent2)\n",
    "NE_tags2=nltk.pos_tag(NE_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "NE_NER1=ne_chunk(NE_tags1)\n",
    "print(NE_NER1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Google/NNP)\n",
      "  's/POS\n",
      "  (ORGANIZATION CEO/NNP Sundar/NNP Pichai/NNP)\n",
      "  introduce/VB\n",
      "  the/DT\n",
      "  new/JJ\n",
      "  Pixel/NNP\n",
      "  3/CD\n",
      "  at/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  Central/NNP\n",
      "  Mall/NNP)\n"
     ]
    }
   ],
   "source": [
    "NE_NER2=ne_chunk(NE_tags2)\n",
    "print(NE_NER2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax\n",
    "\n",
    "In lingustic syntax is the set of rules principal and the processes that govern the structure of a given sentence in a given languages. The terms syntax is also used to refer to a study of such principle and processes. So what we have here are certain rules as to what part of the sentence should come what possition. With these rule, we can create a syntax tree whenerver there is a sentence input.\n",
    "\n",
    "### Syntax Tree\n",
    "\n",
    "Is a representation of syntactic structure of sentences or strings, it is a way of representing the syntax of a programming languages as a hierarchial tree structure. This structure is used for generating symbol tables for compilers and code generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chunking\n",
    "\n",
    "Picking up individual pieces of information and grouping them into bigger pieces, the bigger pieces are also known as chunks. In the contex of NLP and text mining chunking means grouping of words or tokens into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============== Windows only ======================\n",
    "path_to_gs = \"C:\\\\Program Files\\\\gs\\\\gs9.26\\\\bin\\\\\"\n",
    "os.environ['PATH'] += os.pathsep + path_to_gs\n",
    "# ==================================================\n",
    "\n",
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np=r\"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser=nltk.RegexpParser(grammar_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)\n",
    "\n",
    "chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
